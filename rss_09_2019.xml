<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>CoMeT</title>
    <description>Collaborative Management of Talks: A social web system for research communities</description>
    <link>http://halley.exp.sis.pitt.edu/comet/</link>
    <item>
      <title><![CDATA[Democratizing Video Analytics - The quest for the holy trinity of low latency, low cost, and high accuracy]]></title>
      <pubDate>Sunday, Sep 01 10:59 PM -0400</pubDate>
      <link>http://halley.exp.sis.pitt.edu/comet/presentColloquium.do?col_id=17901</link>
      <content:encoded><![CDATA[<b>Speaker:</b>&nbsp;GANESH ANANTHANARAYANAN<br/><b>Date:</b>&nbsp;Thursday, Sep 05 on 12:00 PM - 1:00 PM<br/><b>Location:</b>&nbsp;Robert Mehrabian Collaborative Innovation Center, Panther Hollow Conference Room 4101<br/><b>Detail:<b><p>Video cameras are pervasively deployed for security and smart city scenarios -- there is a camera for every eight people in the US! Large-scale video processing is a grand challenge of systems that enable AI. This talk will describe the recent technical advancements of <a href='http://aka.ms/rocket'>Project Rocket</a>, that takes up this challenge to democratize video analytics: enable anyone with a camera to benefit from video analytics. The first half of the talk will focus on resource management of video processing pipelines. We focus on the key characteristic of video analytics -- resource-accuracy tradeoff with multi-dimensional configurations. We'll present an intelligent profiler that adaptively picks the best configuration without incurring a prohibitive search cost, by leveraging spatial and temporal correlations among the videos. The second part of the talk will move to analyzing stored videos by enabling interactive and low-cost queries of the form, 'find me all frames with object X'. The key to our solution is an approximate index that trades off ingestion cost on live videos against the latency at query time. Project Rocket has been powering our <a href='https://www.youtube.com/watch?v=rTAOwvU6Yj8'>work on traffic video analytics at the City of Bellevue, WA</a> as well as other cities worldwide.</p> <br/> <br/> <p><a href='http://aka.ms/ganesh'>Ganesh Ananthanarayanan</a> is a Researcher at Microsoft Research. His research interests are broadly in systems & networking, with recent focus on live video analytics, cloud computing & large scale data analytics systems, and Internet performance. His work on 'Video Analytics for Vision Zero' on analyzing traffic camera feeds won the Institute of Transportation Engineers 2017 Achievement Award as well as the 'Safer Cities, Safer People' US Department of Transportation Award. He has collaborated with and shipped technology to Microsoft's cloud and online products like the Azure Cloud, Cosmos (Microsoft's big data system) and Skype. He is a member of the ACM Future of Computing Academy. Prior to joining Microsoft Research, he completed his Ph.D. at UC Berkeley in Dec 2013, where he was also a recipient of the Regents Fellowship.</p> <br/> <p><strong>Faculty Host</strong>: Phil Gibbons</p>]]></content:encoded>
      <description><![CDATA[Speaker:�GANESH ANANTHANARAYANANDate:�Thursday, Sep 05 on 12:00 PM - 1:00 PMLocation:�Robert Mehrabian Collaborative Innovation Center, Panther Hollow Conference Room 4101Detail:Video cameras are pervasively deployed for security and smart city scenarios -- there is a camera for every eight people in the US! Large-scale video processing is a grand challenge of systems that enable AI. This talk will describe the recent technical advancements of Project Rocket, that takes up this challenge to democratize video analytics: enable anyone with a camera to benefit from video analytics. The first half of the talk will focus on resource management of video processing pipelines. We focus on the key characteristic of video analytics -- resource-accuracy tradeoff with multi-dimensional configurations. We'll present an intelligent profiler that adaptively picks the best configuration without incurring a prohibitive search cost, by leveraging spatial and temporal correlations among the videos. The second part of the talk will move to analyzing stored videos by enabling interactive and low-cost queries of the form, 'find me all frames with object X'. The key to our solution is an approximate index that trades off ingestion cost on live videos against the latency at query time. Project Rocket has been powering our work on traffic video analytics at the City of Bellevue, WA as well as other cities worldwide. Ganesh Ananthanarayanan is a Researcher at Microsoft Research. His research interests are broadly in systems & networking, with recent focus on live video analytics, cloud computing & large scale data analytics systems, and Internet performance. His work on 'Video Analytics for Vision Zero' on analyzing traffic camera feeds won the Institute of Transportation Engineers 2017 Achievement Award as well as the 'Safer Cities, Safer People' US Department of Transportation Award. He has collaborated with and shipped technology to Microsoft's cloud and online products like the Azure Cloud, Cosmos (Microsoft's big data system) and Skype. He is a member of the ACM Future of Computing Academy. Prior to joining Microsoft Research, he completed his Ph.D. at UC Berkeley in Dec 2013, where he was also a recipient of the Regents Fellowship. Faculty Host: Phil Gibbons]]></description>
      <author>comet.paws</author>
    </item>
    <item>
      <title><![CDATA[Federated Learning, from Research to Practice]]></title>
      <pubDate>Sunday, Sep 01 11:01 PM -0400</pubDate>
      <link>http://halley.exp.sis.pitt.edu/comet/presentColloquium.do?col_id=17902</link>
      <content:encoded><![CDATA[<b>Speaker:</b>&nbsp;BRENDAN McMAHAN<br/><b>Date:</b>&nbsp;Thursday, Sep 05 on 4:00 PM - 5:00 PM<br/><b>Location:</b>&nbsp;Gates Hillman Centers, Traffic21 Classroom 6501<br/><b>Detail:<b><p>Federated Learning enables mobile devices to collaboratively learn a shared prediction model while keeping all the training data on device, decoupling the ability to do machine learning from the need to collect and store the data in a central location. In this talk, I will discuss: how federated learning differs from more traditional machine learning paradigms; practical algorithms for federated learning that address the unique challenges of this setting; extensions to federated learning, including differential privacy, secure aggregation, and compression for model updates; federated learning applications and systems at Google; and finally a selection of exciting open problems and challenges in FL.</p> <br/> <br/> <p><a href='https://ai.google/research/people/author35837'>Brendan McMahan</a> is a research scientist at Google, where he leads efforts on decentralized and privacy-preserving machine learning. His team pioneered the concept of federated learning, and continues to push the boundaries of what is possible when working with decentralized data using privacy-preserving techniques. Previously, he has worked in the fields of online learning, large-scale convex optimization, and reinforcement learning. Brendan received his Ph.D. in computer science from Carnegie Mellon University.</p> <br/> <p><strong>Faculty Host</strong>: Phil Gibbons</p>]]></content:encoded>
      <description><![CDATA[Speaker:�BRENDAN McMAHANDate:�Thursday, Sep 05 on 4:00 PM - 5:00 PMLocation:�Gates Hillman Centers, Traffic21 Classroom 6501Detail:Federated Learning enables mobile devices to collaboratively learn a shared prediction model while keeping all the training data on device, decoupling the ability to do machine learning from the need to collect and store the data in a central location. In this talk, I will discuss: how federated learning differs from more traditional machine learning paradigms; practical algorithms for federated learning that address the unique challenges of this setting; extensions to federated learning, including differential privacy, secure aggregation, and compression for model updates; federated learning applications and systems at Google; and finally a selection of exciting open problems and challenges in FL. Brendan McMahan is a research scientist at Google, where he leads efforts on decentralized and privacy-preserving machine learning. His team pioneered the concept of federated learning, and continues to push the boundaries of what is possible when working with decentralized data using privacy-preserving techniques. Previously, he has worked in the fields of online learning, large-scale convex optimization, and reinforcement learning. Brendan received his Ph.D. in computer science from Carnegie Mellon University. Faculty Host: Phil Gibbons]]></description>
      <author>comet.paws</author>
    </item>
    <item>
      <title><![CDATA[Deep Learning Introduction and Illustrative Applications]]></title>
      <pubDate>Tuesday, Sep 03 11:14 AM -0400</pubDate>
      <link>http://halley.exp.sis.pitt.edu/comet/presentColloquium.do?col_id=17951</link>
      <content:encoded><![CDATA[<b>Speaker:</b>&nbsp;Haohan Wang, MS<br/><b>Date:</b>&nbsp;Friday, Sep 06 on 11:00 AM - 12:00 PM<br/><b>Location:</b>&nbsp;407A/B BAUM, Offices at Baum, 5607 Baum Blvd.<br/><b>Detail:<b><p>TBA</p>]]></content:encoded>
      <description><![CDATA[Speaker:�Haohan Wang, MSDate:�Friday, Sep 06 on 11:00 AM - 12:00 PMLocation:�407A/B BAUM, Offices at Baum, 5607 Baum Blvd.Detail:TBA]]></description>
      <author>tls18</author>
    </item>
    <item>
      <title><![CDATA[Knowledge Transfer Graph for Deep Collaborative Learning]]></title>
      <pubDate>Wednesday, Aug 28 9:57 PM -0400</pubDate>
      <link>http://halley.exp.sis.pitt.edu/comet/presentColloquium.do?col_id=17881</link>
      <content:encoded><![CDATA[<b>Speaker:</b>&nbsp;HIRONOBU FUJIYOSHI<br/><b>Date:</b>&nbsp;Monday, Sep 09 on 3:00 PM - 4:00 PM<br/><b>Location:</b>&nbsp;Newell-Simon Hall, 3305<br/><b>Detail:<b><p>In this talk I will present our latest research about knowledge transfer graph for Deep Collaborative Learning (DCL), which is a method that incorporates Knowledge Distillation and Deep Mutual Learning. DCL is represented by a directional graph where each model is represented by anode, and the propagation of knowledge from the source node to the target node is represented by edges. In DCL, a hyperparameter search can be used to search for an optimal knowledge transfer graph.</p> <br/> <p>?</p> <br/> <p>Dr. <a href='http://mprg.jp/en/'>Hironobu Fujiyoshi</a> received his Ph.D. in Electrical Engineering from Chubu University, Japan, in 1997. From 1997 to 2000 he was a post-doctoral fellow at the Robotics Institute of Carnegie Mellon University, Pittsburgh, PA, USA, working on the DARPA Video Surveillance and Monitoring (VSAM) effort and the humanoid vision project for the HONDA Humanoid Robot. He is now a professor of the Department of Robotics, Chubu University, Japan. From 2005 to 2006, he was a visiting researcher at Robotics Institute, Carnegie Mellon University.</p> <br/> <p><em>Sponsored in part by Facebook Reality Labs</em></p>]]></content:encoded>
      <description><![CDATA[Speaker:�HIRONOBU FUJIYOSHIDate:�Monday, Sep 09 on 3:00 PM - 4:00 PMLocation:�Newell-Simon Hall, 3305Detail:In this talk I will present our latest research about knowledge transfer graph for Deep Collaborative Learning (DCL), which is a method that incorporates Knowledge Distillation and Deep Mutual Learning. DCL is represented by a directional graph where each model is represented by anode, and the propagation of knowledge from the source node to the target node is represented by edges. In DCL, a hyperparameter search can be used to search for an optimal knowledge transfer graph. ? Dr. Hironobu Fujiyoshi received his Ph.D. in Electrical Engineering from Chubu University, Japan, in 1997. From 1997 to 2000 he was a post-doctoral fellow at the Robotics Institute of Carnegie Mellon University, Pittsburgh, PA, USA, working on the DARPA Video Surveillance and Monitoring (VSAM) effort and the humanoid vision project for the HONDA Humanoid Robot. He is now a professor of the Department of Robotics, Chubu University, Japan. From 2005 to 2006, he was a visiting researcher at Robotics Institute, Carnegie Mellon University. Sponsored in part by Facebook Reality Labs]]></description>
      <author>comet.paws</author>
    </item>
    <item>
      <title><![CDATA[Research at Amazon]]></title>
      <pubDate>Sunday, Sep 08 5:10 PM -0400</pubDate>
      <link>http://halley.exp.sis.pitt.edu/comet/presentColloquium.do?col_id=17968</link>
      <content:encoded><![CDATA[<b>Speaker:</b>&nbsp;Alexander Smola<br/><b>Date:</b>&nbsp;Monday, Sep 09 on 4:30 PM - 6:00 PM<br/><b>Location:</b>&nbsp;Gates Hillman Centers Rashid - Auditorium 4401<br/><b>Detail:<b><p>In this talk I will give a sample of some of the research done at AWS. In particular I will talk about some recent results in Reinforcement Learning using a combined on-policy and off-policy approach to obtain rapidly converging and sample efficient algorithms. The key idea in this work is to use propensity scoring and effective sample size reweighting to obtain an optimization algorithm that converges rapidly and that takes advantage of a large replay buffer.</p> <p>Secondly, I will discuss some recent results in multivariate time series prediction. By applying de Finetti&#39;s theorem and causal ordering we are able to obtain a necessary and sufficient&nbsp; characterization of sequence data that consists of a local and global decomposition. The resulting model is computationally efficient and has a high degree of accuracy. Lastly, I will give an update on the D2L.ai project which aims to bring deep learning education to every scientists and engineer.</p> <p>&mdash;</p> <p><a href="https://alex.smola.org/">Alex</a>&nbsp;studied physics at the University of Technology in Munich. After a PhD in computer science at the University of Technology in Berlin in 1998 he worked as researcher at the IDA Group of the GMD. He joined the Australian National University in Canberra in 2000 and NICTA in 2004 where he served as group leader and professor until 2008. From 2008 until 2012 he worked at Yahoo Research and subsequently from 2012 until 2014 at Google Research. He has served as adjunct professor at UC Berkeley in 2011-12 and 2019. In addition to that he was full professor at CMU in 2013-17. After founding Marianas Labs in 2015 he moved to Amazon Web Services in 2016 where he currently serves as VP and Distinguished Scientist. Alex has published over 200 papers and 5 books. His research interests cover kernel methods, Bayesian nonparametrics, large scale inference and deep learning.</p> <p>At Amazon Web Services he helps to build AI and Machine Learning tools for everyone.</p>]]></content:encoded>
      <description><![CDATA[Speaker:�Alexander SmolaDate:�Monday, Sep 09 on 4:30 PM - 6:00 PMLocation:�Gates Hillman Centers Rashid - Auditorium 4401Detail:In this talk I will give a sample of some of the research done at AWS. In particular I will talk about some recent results in Reinforcement Learning using a combined on-policy and off-policy approach to obtain rapidly converging and sample efficient algorithms. The key idea in this work is to use propensity scoring and effective sample size reweighting to obtain an optimization algorithm that converges rapidly and that takes advantage of a large replay buffer. Secondly, I will discuss some recent results in multivariate time series prediction. By applying de Finetti's theorem and causal ordering we are able to obtain a necessary and sufficient� characterization of sequence data that consists of a local and global decomposition. The resulting model is computationally efficient and has a high degree of accuracy. Lastly, I will give an update on the D2L.ai project which aims to bring deep learning education to every scientists and engineer. ? Alex�studied physics at the University of Technology in Munich. After a PhD in computer science at the University of Technology in Berlin in 1998 he worked as researcher at the IDA Group of the GMD. He joined the Australian National University in Canberra in 2000 and NICTA in 2004 where he served as group leader and professor until 2008. From 2008 until 2012 he worked at Yahoo Research and subsequently from 2012 until 2014 at Google Research. He has served as adjunct professor at UC Berkeley in 2011-12 and 2019. In addition to that he was full professor at CMU in 2013-17. After founding Marianas Labs in 2015 he moved to Amazon Web Services in 2016 where he currently serves as VP and Distinguished Scientist. Alex has published over 200 papers and 5 books. His research interests cover kernel methods, Bayesian nonparametrics, large scale inference and deep learning. At Amazon Web Services he helps to build AI and Machine Learning tools for everyone.]]></description>
      <author>Behnam Rahdari</author>
    </item>
    <item>
      <title><![CDATA[Duolingo: Improving Language Learning and Assessment with AI]]></title>
      <pubDate>Wednesday, Aug 28 9:52 PM -0400</pubDate>
      <link>http://halley.exp.sis.pitt.edu/comet/presentColloquium.do?col_id=17873</link>
      <content:encoded><![CDATA[<b>Speaker:</b>&nbsp;Burr Settles<br/><b>Date:</b>&nbsp;Tuesday, Sep 10 on 10:30 AM - 12:00 PM<br/><b>Location:</b>&nbsp;Gates Hillman Centers, ASA Conference Room 6115<br/><b>Detail:<b><p>Consider this paradox: If education is the key to getting out of poverty and achieving greater social equality, why is high-quality education so often expensive and inaccessible to those who need it most? Artificial intelligence will never replace great teachers, but until everyone in the world has equal access to great teachers, AI and machine learning provide a tremendous opportunity to scale quality education to everyone who needs it.<br> <br> In this talk, I will describe Duolingo?s mission to combine AI-driven learning technology with a unique business model, enabling us to provide language instruction for free and bring us closer to a more engaged, empathetic, and educated world. Duolingo has more than 300 million users from virtually every country in the world. Their lesson data can be harnessed to develop novel technologies, such as personalized practice sessions and data-driven language assessments like the Duolingo English Test (DET). These efforts combine learner data with machine learning, computational linguistics, and psychometrics to improve educational and motivational outcomes, crossing socioeconomic barriers from America to Zimbabwe.</p> <br/> <p>?</p> <br/> <p><a href='http://burrsettles.com/'>Burr Settles</a> is Research Director at <a href='https://ai.duolingo.com/'>Duolingo</a>, the world?s largest language-learning platform. He is also the author of <em>Active Learning</em>, a text on adaptive machine learning algorithms. His research has been published in major AI venues such as Cognitive Science, NeurIPS, ICML, and AAAI, and has been covered by <em>The New York Times</em>, <em>Forbes</em>, <em>WIRED</em>, and BBC. Previously, Burr was a postdoc at Carnegie Mellon and earned a PhD from UW-Madison.</p> <br/> <p><em>The ML Seminar is generously supported by Duolingo.</em></p>]]></content:encoded>
      <description><![CDATA[Speaker:�Burr SettlesDate:�Tuesday, Sep 10 on 10:30 AM - 12:00 PMLocation:�Gates Hillman Centers, ASA Conference Room 6115Detail:Consider this paradox: If education is the key to getting out of poverty and achieving greater social equality, why is high-quality education so often expensive and inaccessible to those who need it most? Artificial intelligence will never replace great teachers, but until everyone in the world has equal access to great teachers, AI and machine learning provide a tremendous opportunity to scale quality education to everyone who needs it. In this talk, I will describe Duolingo?s mission to combine AI-driven learning technology with a unique business model, enabling us to provide language instruction for free and bring us closer to a more engaged, empathetic, and educated world. Duolingo has more than 300 million users from virtually every country in the world. Their lesson data can be harnessed to develop novel technologies, such as personalized practice sessions and data-driven language assessments like the Duolingo English Test (DET). These efforts combine learner data with machine learning, computational linguistics, and psychometrics to improve educational and motivational outcomes, crossing socioeconomic barriers from America to Zimbabwe. ? Burr Settles is Research Director at Duolingo, the world?s largest language-learning platform. He is also the author of Active Learning, a text on adaptive machine learning algorithms. His research has been published in major AI venues such as Cognitive Science, NeurIPS, ICML, and AAAI, and has been covered by The New York Times, Forbes, WIRED, and BBC. Previously, Burr was a postdoc at Carnegie Mellon and earned a PhD from UW-Madison. The ML Seminar is generously supported by Duolingo.]]></description>
      <author>comet.paws</author>
    </item>
    <item>
      <title><![CDATA[AI in Space - From Earth Orbit to Mars and Beyond!]]></title>
      <pubDate>Tuesday, Sep 10 12:04 PM -0400</pubDate>
      <link>http://halley.exp.sis.pitt.edu/comet/presentColloquium.do?col_id=17952</link>
      <content:encoded><![CDATA[<b>Speaker:</b>&nbsp;Jagriti Agrawal<br/><b>Date:</b>&nbsp;Tuesday, Sep 10 on 4:00 PM - 5:00 PM<br/><b>Location:</b>&nbsp;3305 Newell-Simon Hall, CMU<br/><b>Detail:<b><p><strong>Abstract:</strong></p> <p>Artificial Intelligence is playing an increasing role in our everyday lives and the business marketplace. This trend extends to the space sector, where AI has already shown considerable success and has the potential to revolutionize almost every aspect of space exploration.</p> <p>We first highlight a number of success stories of the tremendous impact of Artificial Intelligence in Space: over a dozen years of operations of the Autonomous Sciencecraft on EO-1, the Earth Observing Sensorweb tracking volcanoes, flooding and wildfires and automated targeting onboard the MER and MSL rovers.</p> <p>Next we describe how AI-based scheduling is being deployed to NASA&rsquo;s next rover to Mars, the M2020 rover.</p> <p>Finally we discuss why AI is essential to the search for life beyond Earth, highlighting the key role of AI in Europa Submersible and Interstellar mission concepts.</p> <p><strong>Bio:</strong></p> <p>Dr. Steve Chien is a Senior Research Scientist at the Jet Propulsion Laboratory, California Institute of Technology where he leads efforts in autonomous systems for space exploration. Dr. Chien has received numerous awards for his research in space autonomous systems including: NASA Medals in 1997, 2000, 2007, and 2015; he is a four time honoree in the NASA Software of the Year competition (1999, 1999, 2005, 2011); and in 2011 he was awarded the inaugural AIAA Intelligent Systems Award. He has led the deployment of ground and flight autonomy software to numerous missions including the Autonomous Sciencecraft/Earth Observing One, WATCH/Mars Exploration Rovers, Earth Observing Sensorwebs, IPEX, ESA&rsquo;s Rosetta, ECOSTRESS and OCO-3 missions and is currently contributing to onboard and ground scheduling for the M2020 rover mission.</p> <p>Ms. Jagriti Agrawal is a Member of Technical Staff &nbsp;in the Artificial Intelligence Group at the Jet Propulsion Laboratory, California Institute of Technology where she works on automated scheduling for the upcoming M2020 Mars Rover mission.</p>]]></content:encoded>
      <description><![CDATA[Speaker:�Jagriti AgrawalDate:�Tuesday, Sep 10 on 4:00 PM - 5:00 PMLocation:�3305 Newell-Simon Hall, CMUDetail:Abstract: Artificial Intelligence is playing an increasing role in our everyday lives and the business marketplace. This trend extends to the space sector, where AI has already shown considerable success and has the potential to revolutionize almost every aspect of space exploration. We first highlight a number of success stories of the tremendous impact of Artificial Intelligence in Space: over a dozen years of operations of the Autonomous Sciencecraft on EO-1, the Earth Observing Sensorweb tracking volcanoes, flooding and wildfires and automated targeting onboard the MER and MSL rovers. Next we describe how AI-based scheduling is being deployed to NASA?s next rover to Mars, the M2020 rover. Finally we discuss why AI is essential to the search for life beyond Earth, highlighting the key role of AI in Europa Submersible and Interstellar mission concepts. Bio: Dr. Steve Chien is a Senior Research Scientist at the Jet Propulsion Laboratory, California Institute of Technology where he leads efforts in autonomous systems for space exploration. Dr. Chien has received numerous awards for his research in space autonomous systems including: NASA Medals in 1997, 2000, 2007, and 2015; he is a four time honoree in the NASA Software of the Year competition (1999, 1999, 2005, 2011); and in 2011 he was awarded the inaugural AIAA Intelligent Systems Award. He has led the deployment of ground and flight autonomy software to numerous missions including the Autonomous Sciencecraft/Earth Observing One, WATCH/Mars Exploration Rovers, Earth Observing Sensorwebs, IPEX, ESA?s Rosetta, ECOSTRESS and OCO-3 missions and is currently contributing to onboard and ground scheduling for the M2020 rover mission. Ms. Jagriti Agrawal is a Member of Technical Staff �in the Artificial Intelligence Group at the Jet Propulsion Laboratory, California Institute of Technology where she works on automated scheduling for the upcoming M2020 Mars Rover mission.]]></description>
      <author>bmaneesh</author>
    </item>
    <item>
      <title><![CDATA[BLOOMBERG Graduate Tech Talk: AI at Bloomberg]]></title>
      <pubDate>Tuesday, Sep 10 3:39 PM -0400</pubDate>
      <link>http://halley.exp.sis.pitt.edu/comet/presentColloquium.do?col_id=18009</link>
      <content:encoded><![CDATA[<b>Speaker:</b>&nbsp;Drs. WOOYOUNG LEE and ANDREW HSI<br/><b>Date:</b>&nbsp;Thursday, Sep 12 on 12:00 PM - 1:30 PM<br/><b>Location:</b>&nbsp;Gates Hillman Centers, Reddy Conference Room 4405<br/><b>Detail:<b><p> (PhD and MS Only) Decision makers across the global finance industry look to Bloomberg to deliver timely political, business and financial news, information and insight. In this talk, <strong>Dr. Wooyoung Lee </strong>(MLD 2014) and<strong> Dr. Andrew Hsi</strong> (LTI 2018) will provide an overview of Bloomberg's cutting-edge AI research and touch on the machine learning and natural language processing capabilities and infrastructure that Bloomberg?s AI team have built.</p> <br/> <p>Please <a href='http://tinyurl.com/y5tuuofk'>REGISTER</a></p>]]></content:encoded>
      <description><![CDATA[Speaker:�Drs. WOOYOUNG LEE and ANDREW HSIDate:�Thursday, Sep 12 on 12:00 PM - 1:30 PMLocation:�Gates Hillman Centers, Reddy Conference Room 4405Detail: (PhD and MS Only) Decision makers across the global finance industry look to Bloomberg to deliver timely political, business and financial news, information and insight. In this talk, Dr. Wooyoung Lee (MLD 2014) and Dr. Andrew Hsi (LTI 2018) will provide an overview of Bloomberg's cutting-edge AI research and touch on the machine learning and natural language processing capabilities and infrastructure that Bloomberg?s AI team have built. Please REGISTER]]></description>
      <author>comet.paws</author>
    </item>
    <item>
      <title><![CDATA[Recent Advances in Biomedical Question Answering]]></title>
      <pubDate>Friday, Sep 06 11:38 AM -0400</pubDate>
      <link>http://halley.exp.sis.pitt.edu/comet/presentColloquium.do?col_id=17962</link>
      <content:encoded><![CDATA[<b>Speaker:</b>&nbsp;Eric Nyberg, PhD<br/><b>Date:</b>&nbsp;Friday, Sep 13 on 11:00 AM - 12:00 PM<br/><b>Location:</b>&nbsp;5607 Baum Boulevard, Room 407A<br/><b>Detail:<b><p>This talk presents recent results on automatic question answering for biomedical and consumer health questions, drawn from CMU&#39;s participation in several recent open evaluations (including BioASQ, LiveQA, and MediQA). Despite promising achievements in recent evaluation campaigns, significant challenges must be solved before QA can be practically useful for experts and consumers. We analyze the strengths and weaknesses of current (and past) approaches, and offer suggestions for how the QA field can mature toward generalized QA performance.</p> <p>Presenter&#39;s Website:&nbsp;<a href="https://www.cs.cmu.edu/~ehn/">https://www.cs.cmu.edu/~ehn/</a></p> <p>Noted for his contributions to the fields of automatic text translation, information retrieval, and automatic question answering, Nyberg holds a Ph.D. from Carnegie Mellon University (1992) and a B.A. from Boston University (1983). He is a recipient of the&nbsp;<a href="https://www.scs.cmu.edu/people/achievements/newell" target="new_window">Allen Newell Award for Research Excellence</a>&nbsp;(for his contributions to the field of question answering and his work as an original developer on the&nbsp;<a href="https://en.wikipedia.org/wiki/Watson_(computer)" target="new_window">Watson project</a>) and the&nbsp;<a href="http://www.bu.edu/cs/people/alumni/bu-computing-alumni-network/alumni-award/2013-award/" target="new_window">BU Computer Science Distinguished Alumna/Alumnus Award</a>. Eric currently directs the&nbsp;<a href="http://mcds.cs.cmu.edu/" target="new_window">Master of Computational Data Science (MCDS) program</a>. He is also co-Founder and Chief Data Scientist at&nbsp;<a href="http://www.cognistx.com/" target="new_window">Cognistx</a>, and serves on the Scientific Advisory Board for&nbsp;<a href="https://www.meltwater.com/press/meltwater-launches-data-science-platform-fairhair-ai/" target="new_window">Fairhair.ai</a>.</p> <p>&nbsp;</p>]]></content:encoded>
      <description><![CDATA[Speaker:�Eric Nyberg, PhDDate:�Friday, Sep 13 on 11:00 AM - 12:00 PMLocation:�5607 Baum Boulevard, Room 407ADetail:This talk presents recent results on automatic question answering for biomedical and consumer health questions, drawn from CMU's participation in several recent open evaluations (including BioASQ, LiveQA, and MediQA). Despite promising achievements in recent evaluation campaigns, significant challenges must be solved before QA can be practically useful for experts and consumers. We analyze the strengths and weaknesses of current (and past) approaches, and offer suggestions for how the QA field can mature toward generalized QA performance. Presenter's Website:�https://www.cs.cmu.edu/~ehn/ Noted for his contributions to the fields of automatic text translation, information retrieval, and automatic question answering, Nyberg holds a Ph.D. from Carnegie Mellon University (1992) and a B.A. from Boston University (1983). He is a recipient of the�Allen Newell Award for Research Excellence�(for his contributions to the field of question answering and his work as an original developer on the�Watson project) and the�BU Computer Science Distinguished Alumna/Alumnus Award. Eric currently directs the�Master of Computational Data Science (MCDS) program. He is also co-Founder and Chief Data Scientist at�Cognistx, and serves on the Scientific Advisory Board for�Fairhair.ai. �]]></description>
      <author>bjw71</author>
    </item>
    <item>
      <title><![CDATA[The case for self-sovereign personal AI]]></title>
      <pubDate>Monday, Sep 16 3:53 PM -0400</pubDate>
      <link>http://halley.exp.sis.pitt.edu/comet/presentColloquium.do?col_id=17826</link>
      <content:encoded><![CDATA[<b>Speaker:</b>&nbsp;Adrian Gropper, MD<br/><b>Date:</b>&nbsp;Friday, Sep 13 on 1:30 PM - 2:30 PM<br/><b>Location:</b>&nbsp;Newell-Simon Hall 1305 (Michael Mauldin Auditorium)<br/><b>Detail:<b><p><strong>Description</strong></p> <p>Whether it&rsquo;s a smartphone that filters notifications or a brain implant that manages a neurological problem, connected personal technology tests the definition and limits of &ldquo;self.&rdquo; Our human identity is a combination of attributes managed by ourselves and attributes that relate to us but are managed by others, often without our knowledge or informed consent.</p> <p>Today, there is no practical way to evaluate requests for use of our personal data, nor clear solution for designers who wish to incorporate privacy management at the core of their products and services. At a time when platforms such as Facebook are under scrutiny for how they use personal data, a self-sovereign agent can begin to rectify the vast asymmetry of power between the platform and the data subject.</p> <p>Self-sovereign agent technology has a fiduciary relationship with the human subject that mirrors the relationship a physician has with a patient. Identity that is exposed by a self-sovereign agent is designed to minimize the leakage of personal information as individuals engage with other people and entities, both public and private. However, the cost of managing personal information &ldquo;manually&rdquo; for each of the dozens and hundreds of entities we encounter every week is unsustainable. Therefore, individuals need a way to automate the vast majority of decisions that are directly linked to their identity, inviting the integration of AI as a decision agent into the design of human-computer systems. Machine learning could balance the social intent with the privacy of interactions that we typically reserve for our real-time, conscious selves in a conversation.</p> <p>It could be argued that privacy is now a limiting factor in computer science. This talk is about the interaction between self-sovereign identity and self-sovereign technology standards using examples from health records and &ldquo;connected home&rdquo; IoT devices.</p> <p><strong>Speaker&#39;s Bio</strong></p> <p>Adrian Gropper, MD is CTO of the non-profit Patient Privacy Rights Foundation where he brings training as an engineer from MIT and physician from Harvard Medical School followed by a career as a medical device entrepreneur. He founded three regulated medical diagnostics businesses including AMICAS, the first Web-based radiology image network and the first to provide imaging links in electronic health records. He participated in the founding of many healthcare interoperability initiatives including Blue Button, Direct Project, and Health Relationship Trust (HEART) and he speaks frequently on privacy engineering in health care. His paper won a prize at ONC&rsquo;s 2016 Blockchain Health competition. His current project, <a href="https://dir.hieofone.org/welcome0">Trustee by HIE of One</a> (Health Information Exchange of One), uses public blockchains, standards, and free software to enable patient-controlled independent health records that can span a lifetime. This reference implementation informs emerging blockchain standards development for identity, credentials, and reputation with groups that include W3C, IEEE, Kantara, OpenID Foundation, and others.&nbsp; Website: <a href="http://healthurl.com/">http://healthurl.com/</a></p>]]></content:encoded>
      <description><![CDATA[Speaker:�Adrian Gropper, MDDate:�Friday, Sep 13 on 1:30 PM - 2:30 PMLocation:�Newell-Simon Hall 1305 (Michael Mauldin Auditorium)Detail:Description Whether it?s a smartphone that filters notifications or a brain implant that manages a neurological problem, connected personal technology tests the definition and limits of ?self.? Our human identity is a combination of attributes managed by ourselves and attributes that relate to us but are managed by others, often without our knowledge or informed consent. Today, there is no practical way to evaluate requests for use of our personal data, nor clear solution for designers who wish to incorporate privacy management at the core of their products and services. At a time when platforms such as Facebook are under scrutiny for how they use personal data, a self-sovereign agent can begin to rectify the vast asymmetry of power between the platform and the data subject. Self-sovereign agent technology has a fiduciary relationship with the human subject that mirrors the relationship a physician has with a patient. Identity that is exposed by a self-sovereign agent is designed to minimize the leakage of personal information as individuals engage with other people and entities, both public and private. However, the cost of managing personal information ?manually? for each of the dozens and hundreds of entities we encounter every week is unsustainable. Therefore, individuals need a way to automate the vast majority of decisions that are directly linked to their identity, inviting the integration of AI as a decision agent into the design of human-computer systems. Machine learning could balance the social intent with the privacy of interactions that we typically reserve for our real-time, conscious selves in a conversation. It could be argued that privacy is now a limiting factor in computer science. This talk is about the interaction between self-sovereign identity and self-sovereign technology standards using examples from health records and ?connected home? IoT devices. Speaker's Bio Adrian Gropper, MD is CTO of the non-profit Patient Privacy Rights Foundation where he brings training as an engineer from MIT and physician from Harvard Medical School followed by a career as a medical device entrepreneur. He founded three regulated medical diagnostics businesses including AMICAS, the first Web-based radiology image network and the first to provide imaging links in electronic health records. He participated in the founding of many healthcare interoperability initiatives including Blue Button, Direct Project, and Health Relationship Trust (HEART) and he speaks frequently on privacy engineering in health care. His paper won a prize at ONC?s 2016 Blockchain Health competition. His current project, Trustee by HIE of One (Health Information Exchange of One), uses public blockchains, standards, and free software to enable patient-controlled independent health records that can span a lifetime. This reference implementation informs emerging blockchain standards development for identity, credentials, and reputation with groups that include W3C, IEEE, Kantara, OpenID Foundation, and others.� Website: http://healthurl.com/]]></description>
      <author>CMU HCII</author>
    </item>
    <item>
      <title><![CDATA[Interface of Statistics and Computing]]></title>
      <pubDate>Monday, Sep 16 6:18 AM -0400</pubDate>
      <link>http://halley.exp.sis.pitt.edu/comet/presentColloquium.do?col_id=18022</link>
      <content:encoded><![CDATA[<b>Speaker:</b>&nbsp;Yazhen Wang<br/><b>Date:</b>&nbsp;Monday, Sep 16 on 4:00 PM - 5:00 PM<br/><b>Location:</b>&nbsp;Scaife Hall 125<br/><b>Detail:<b><p>The role of computation has been central to statistics for decades, and statistics and computing become ever more important in the age of data science.&nbsp;&nbsp;This talk will present the interface of statistics and computing through my statistics work on computational algorithms and computing work on statistics and machine learning.&nbsp;&nbsp;Specifically I will discuss (i) quantum computing in particular annealing based quantum computing for deep learning; (ii) statistical analysis of gradient descent (accelerated gradient descent and stochastic gradient descent algorithms) in the context of stochastic optimization arising in statistics and machine learning where objective functions are estimated from available data.</p> <p>Dr.&nbsp;<a href="http://pages.stat.wisc.edu/~yzwang/">Yazhen Wang</a>&nbsp;is Professor of Statistics at the University of Wisconsin-Madison and served as the department chair from 2015-2018. He obtained his Ph.D in statistics from the University of California at Berkeley in 1992. He is a fellow of the ASA and IMS. He has served as NSF program director, various committees of ASA, IMS and ICSA; editor of Statistica Sinica and Statistics and Its Interface; associate editor of Annals of Statistics, Annals of Applied Statistics, Journal of the American Statistical Association, Journal of Business &amp; Economic Statistics, Statistica Sinica, and the Econometrics Journal. His research areas include financial econometrics, statistical machine learning, quantum computation, high dimensional statistical inference, nonparametric curve estimation, wavelets and multi-scale methods, change points, long-memory processes, and order restricted inference.</p>]]></content:encoded>
      <description><![CDATA[Speaker:�Yazhen WangDate:�Monday, Sep 16 on 4:00 PM - 5:00 PMLocation:�Scaife Hall 125Detail:The role of computation has been central to statistics for decades, and statistics and computing become ever more important in the age of data science.��This talk will present the interface of statistics and computing through my statistics work on computational algorithms and computing work on statistics and machine learning.��Specifically I will discuss (i) quantum computing in particular annealing based quantum computing for deep learning; (ii) statistical analysis of gradient descent (accelerated gradient descent and stochastic gradient descent algorithms) in the context of stochastic optimization arising in statistics and machine learning where objective functions are estimated from available data. Dr.�Yazhen Wang�is Professor of Statistics at the University of Wisconsin-Madison and served as the department chair from 2015-2018. He obtained his Ph.D in statistics from the University of California at Berkeley in 1992. He is a fellow of the ASA and IMS. He has served as NSF program director, various committees of ASA, IMS and ICSA; editor of Statistica Sinica and Statistics and Its Interface; associate editor of Annals of Statistics, Annals of Applied Statistics, Journal of the American Statistical Association, Journal of Business & Economic Statistics, Statistica Sinica, and the Econometrics Journal. His research areas include financial econometrics, statistical machine learning, quantum computation, high dimensional statistical inference, nonparametric curve estimation, wavelets and multi-scale methods, change points, long-memory processes, and order restricted inference.]]></description>
      <author>OliverHao</author>
    </item>
    <item>
      <title><![CDATA[Deep Learning for AI]]></title>
      <pubDate>Thursday, Sep 12 11:47 AM -0400</pubDate>
      <link>http://halley.exp.sis.pitt.edu/comet/presentColloquium.do?col_id=18016</link>
      <content:encoded><![CDATA[<b>Speaker:</b>&nbsp;Yoshua Bengio<br/><b>Date:</b>&nbsp;Monday, Sep 23 on 3:00 AM - 3:45 AM<br/><b>Location:</b>&nbsp;Online<br/><b>Detail:<b><p>2018 ACM A.M. Turing Award Laureate Yoshua Bengio will deliver his Turing Lecture at the Heidelberg Laureate Forum (HLF). Bengio&rsquo;s talk, titled, &quot;Deep Learning for AI,&quot; will be presented on Monday, September 23 from 9:00 - 9:45 a.m. Central European Summer Time. The Turing Lecture will be livestreamed on the HLF website at <a href="http://www.heidelberg-laureate-forum.org">www.heidelberg-laureate-forum.org/</a>.</p> <p>Bengio received the 2018 ACM A.M. Turing Award with Geoffrey Hinton and Yann LeCun for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing.</p>]]></content:encoded>
      <description><![CDATA[Speaker:�Yoshua BengioDate:�Monday, Sep 23 on 3:00 AM - 3:45 AMLocation:�OnlineDetail:2018 ACM A.M. Turing Award Laureate Yoshua Bengio will deliver his Turing Lecture at the Heidelberg Laureate Forum (HLF). Bengio?s talk, titled, "Deep Learning for AI," will be presented on Monday, September 23 from 9:00 - 9:45 a.m. Central European Summer Time. The Turing Lecture will be livestreamed on the HLF website at www.heidelberg-laureate-forum.org/. Bengio received the 2018 ACM A.M. Turing Award with Geoffrey Hinton and Yann LeCun for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing.]]></description>
      <author>chirayu</author>
    </item>
    <item>
      <title><![CDATA[Thesis Defense: Designing Real-time Teacher Augmentation to Combine Strengths of Human and AI Instruction]]></title>
      <pubDate>Wednesday, Sep 04 11:07 PM -0400</pubDate>
      <link>http://halley.exp.sis.pitt.edu/comet/presentColloquium.do?col_id=17882</link>
      <content:encoded><![CDATA[<b>Speaker:</b>&nbsp;Kenneth Holstein<br/><b>Date:</b>&nbsp;Tuesday, Sep 24 on 10:00 AM - 12:00 PM<br/><b>Location:</b>&nbsp;Gates-Hillman Center (GHC) 6115<br/><b>Detail:<b><p><strong>Committee:</strong></p> <p>Vincent Aleven, Co-Chair (HCII, CMU)<br /> Bruce M. McLaren, Co-Chair (HCII, CMU)<br /> Jodi&nbsp;Forlizzi (HCII, CMU)<br /> Pierre Dillenbourg (School of Computer and Communication Sciences, EPFL)</p> <p>Nikol Rummel (Department of Psychology, RUB &amp; HCII, CMU)</p> <p>&nbsp;</p> <p><strong>Abstract:</strong></p> <p>When used in K-12 classrooms, AI-based educational software such as intelligent tutoring systems (ITSs) allows students to work at their own pace, while also freeing up the teacher to spend more time working one-on-one with students. A common intuition is that, in many situations, human teachers may be better suited to support students than ITSs alone (e.g., by providing socio-emotional support, supporting student motivation, or flexibly providing conceptual support when further problem-solving practice may be ineffective). Yet ITSs are not typically designed to work together with teachers during a class session, to take advantage of these complementary strengths.&nbsp;</p> <p>This dissertation explores how AI tutors might be better designed to work together with human teachers in real-time, to amplify teachers&rsquo; abilities to help their students. Working together with 36 middle school math teachers, I conducted the first broad exploration in the literature of teachers&rsquo; needs for real-time analytics and orchestration support in AI-supported, personalized classrooms. As part of this work, I worked with teachers to design a form of real-time, wearable teacher augmentation called <em>Lumilo</em>.&nbsp;</p> <p><em>Lumilo </em>is a set of mixed-reality smart glasses that direct teachers&rsquo; attention during a class session, towards situations the tutoring software may be ill-suited to handle on its own, and support teachers in deciding how best to respond.<em> Lumilo</em> has been used by teachers and students in over 40 middle school classrooms so far. An in-vivo classroom experiment showed that teacher&ndash;AI co-orchestration, as supported by <em>Lumilo</em>, enhanced students&rsquo; learning compared with an AI-supported classroom in which the teacher did not have such support.&nbsp;</p> <p>Over the course of this research, I have also developed new design and prototyping methods to address challenges in the co-design, experience prototyping, and evaluation of data-driven AI systems. To support the use of these methods within the area of education, my collaborators and I have extended an existing technical architecture (<em>CTAT/TutorShop</em>) to facilitate rapid prototyping of data-driven educational AI applications.</p> <p>In the final chapters of this dissertation, I explore how the concepts embodied by <em>Lumilo</em> might be prepared for wider use, from two angles. First, I involve students, as well as teachers, in the next phase of design to better serve the needs and respect the boundaries of both stakeholder groups. Second, through a newly-formed academic&ndash;industry partnership with Carnegie Learning (a major educational AI company) I begin to explore how real-time, wearable teacher augmentation might be generalized to work with a broader range of AI tutoring systems and curricula.</p> <p>&nbsp;</p>]]></content:encoded>
      <description><![CDATA[Speaker:�Kenneth HolsteinDate:�Tuesday, Sep 24 on 10:00 AM - 12:00 PMLocation:�Gates-Hillman Center (GHC) 6115Detail:Committee: Vincent Aleven, Co-Chair (HCII, CMU) Bruce M. McLaren, Co-Chair (HCII, CMU) Jodi�Forlizzi (HCII, CMU) Pierre Dillenbourg (School of Computer and Communication Sciences, EPFL) Nikol Rummel (Department of Psychology, RUB & HCII, CMU) � Abstract: When used in K-12 classrooms, AI-based educational software such as intelligent tutoring systems (ITSs) allows students to work at their own pace, while also freeing up the teacher to spend more time working one-on-one with students. A common intuition is that, in many situations, human teachers may be better suited to support students than ITSs alone (e.g., by providing socio-emotional support, supporting student motivation, or flexibly providing conceptual support when further problem-solving practice may be ineffective). Yet ITSs are not typically designed to work together with teachers during a class session, to take advantage of these complementary strengths.� This dissertation explores how AI tutors might be better designed to work together with human teachers in real-time, to amplify teachers? abilities to help their students. Working together with 36 middle school math teachers, I conducted the first broad exploration in the literature of teachers? needs for real-time analytics and orchestration support in AI-supported, personalized classrooms. As part of this work, I worked with teachers to design a form of real-time, wearable teacher augmentation called Lumilo.� Lumilo is a set of mixed-reality smart glasses that direct teachers? attention during a class session, towards situations the tutoring software may be ill-suited to handle on its own, and support teachers in deciding how best to respond. Lumilo has been used by teachers and students in over 40 middle school classrooms so far. An in-vivo classroom experiment showed that teacher?AI co-orchestration, as supported by Lumilo, enhanced students? learning compared with an AI-supported classroom in which the teacher did not have such support.� Over the course of this research, I have also developed new design and prototyping methods to address challenges in the co-design, experience prototyping, and evaluation of data-driven AI systems. To support the use of these methods within the area of education, my collaborators and I have extended an existing technical architecture (CTAT/TutorShop) to facilitate rapid prototyping of data-driven educational AI applications. In the final chapters of this dissertation, I explore how the concepts embodied by Lumilo might be prepared for wider use, from two angles. First, I involve students, as well as teachers, in the next phase of design to better serve the needs and respect the boundaries of both stakeholder groups. Second, through a newly-formed academic?industry partnership with Carnegie Learning (a major educational AI company) I begin to explore how real-time, wearable teacher augmentation might be generalized to work with a broader range of AI tutoring systems and curricula. �]]></description>
      <author>comet.paws</author>
    </item>
    <item>
      <title><![CDATA[Data Driven Algorithm Design]]></title>
      <pubDate>Sunday, Sep 22 3:19 PM -0400</pubDate>
      <link>http://halley.exp.sis.pitt.edu/comet/presentColloquium.do?col_id=18058</link>
      <content:encoded><![CDATA[<b>Speaker:</b>&nbsp;Nina Balcan<br/><b>Date:</b>&nbsp;Tuesday, Sep 24 on 10:00 AM - 11:00 AM<br/><b>Location:</b>&nbsp;Newell-Simon Hall, 4305<br/><b>Detail:<b><p>Data driven algorithm design for combinatorial problems is an important aspect of modern data science and algorithm design. Rather than using off the shelf algorithms that only have worst case performance guarantees, practitioners typically optimize over large families of parametrized algorithms and tune the parameters of these algorithms using a training set of problem instances from their domain to determine a configuration with high expected performance over future instances. However, most of this work comes with no performance guarantees. The challenge is that for many combinatorial problems, including partitioning and subset selection problems, a small tweak to the parameters can cause a cascade of changes in the algorithm&#39;s behavior, so the algorithm&#39;s performance is a discontinuous function of its parameters.</p> <p>In this talk, I will present new work that helps put data driven combinatorial algorithm selection on firm foundations. &nbsp;This includes strong computational and statistical performance guarantees, both for the batch and online scenarios where a collection of typical problem instances from the given application are presented either all at once or in an online fashion, respectively. &nbsp;I will describe both specific examples (for clustering, partitioning, and subset selection problems) and&nbsp;general principles that emerge in this context.</p> <p><em>The Machine Learning Seminar is generously supported by <a href="http://www.duolingo.com">Duolingo</a></em></p> <p><em>This talk will be recorded and posted on the ML website</em></p>]]></content:encoded>
      <description><![CDATA[Speaker:�Nina BalcanDate:�Tuesday, Sep 24 on 10:00 AM - 11:00 AMLocation:�Newell-Simon Hall, 4305Detail:Data driven algorithm design for combinatorial problems is an important aspect of modern data science and algorithm design. Rather than using off the shelf algorithms that only have worst case performance guarantees, practitioners typically optimize over large families of parametrized algorithms and tune the parameters of these algorithms using a training set of problem instances from their domain to determine a configuration with high expected performance over future instances. However, most of this work comes with no performance guarantees. The challenge is that for many combinatorial problems, including partitioning and subset selection problems, a small tweak to the parameters can cause a cascade of changes in the algorithm's behavior, so the algorithm's performance is a discontinuous function of its parameters. In this talk, I will present new work that helps put data driven combinatorial algorithm selection on firm foundations. �This includes strong computational and statistical performance guarantees, both for the batch and online scenarios where a collection of typical problem instances from the given application are presented either all at once or in an online fashion, respectively. �I will describe both specific examples (for clustering, partitioning, and subset selection problems) and�general principles that emerge in this context. The Machine Learning Seminar is generously supported by Duolingo This talk will be recorded and posted on the ML website]]></description>
      <author>comet.paws</author>
    </item>
    <item>
      <title><![CDATA[Association and Imagination]]></title>
      <pubDate>Wednesday, Sep 18 5:30 AM -0400</pubDate>
      <link>http://halley.exp.sis.pitt.edu/comet/presentColloquium.do?col_id=18044</link>
      <content:encoded><![CDATA[<b>Speaker:</b>&nbsp;Aayush Bansal<br/><b>Date:</b>&nbsp;Tuesday, Sep 24 on 12:00 PM - 1:00 PM<br/><b>Location:</b>&nbsp;NSH 3305, CMU<br/><b>Detail:<b><p><strong>Abstract: </strong>When you see the sunset standing on Seine river in Paris, you can trivially imagine it along the Monongahela in Pittsburgh. When you hear something, you can easily imagine how someone would have said it. When you think of an event from the past, you can relive every bit of it in your imagination. Humans have remarkable abilities to associate different concepts and create visual worlds far beyond what could be seen by a human eye, including inferring the state of unobserved, imagining the unknown, and thinking about diverse possibilities about what lies in the future. These human powers require minimal instructions, and primarily relies on observation and interaction with a dynamic environment. The simple tasks from daily life that are trivial for humans to think and imagine have remained challenging for machine perception and artificial intelligence. The inability to associate and a lack of sense of imagination in machines substantially restricts their applicability.</p> <p>In this talk, I will demonstrate how thinking about association at various levels of abstraction can lead to machine imagination. I will present algorithms that enable association between different domains in an unsupervised manner. This ability to associate allows automatic creation of audio and visual content (images, videos, 4D space-time visualization of dynamic events) that is also user-controllable and interactive. I will show diverse user applications in audio-visual data retargeting, reconstruction, synthesis, and manipulation. These applications are the first steps towards building machines with a powerful audio-visual simulator that will enable them to imagine complex hypothetical situations, and model the aspects of their surroundings that is not easily perceived.</p> <p><strong>Bio</strong> - Aayush Bansal is a PhD candidate at the Robotics Institute of Carnegie Mellon University. He is a recipient of Uber Presidential Fellowship (2016-17), Qualcomm Fellowship (2017-18), and Snap Fellowship (2019-20). The production houses such as BBC Studios and PBS are using his research work to create documentaries and short movies. Various national and international media such as NBC, CBS, France TV, and The Journalist have extensively covered his work. More details are here: <a href="http://www.cs.cmu.edu/~aayushb" target="_blank">http://www.cs.cmu.edu/~aayushb</a></p>]]></content:encoded>
      <description><![CDATA[Speaker:�Aayush BansalDate:�Tuesday, Sep 24 on 12:00 PM - 1:00 PMLocation:�NSH 3305, CMUDetail:Abstract: When you see the sunset standing on Seine river in Paris, you can trivially imagine it along the Monongahela in Pittsburgh. When you hear something, you can easily imagine how someone would have said it. When you think of an event from the past, you can relive every bit of it in your imagination. Humans have remarkable abilities to associate different concepts and create visual worlds far beyond what could be seen by a human eye, including inferring the state of unobserved, imagining the unknown, and thinking about diverse possibilities about what lies in the future. These human powers require minimal instructions, and primarily relies on observation and interaction with a dynamic environment. The simple tasks from daily life that are trivial for humans to think and imagine have remained challenging for machine perception and artificial intelligence. The inability to associate and a lack of sense of imagination in machines substantially restricts their applicability. In this talk, I will demonstrate how thinking about association at various levels of abstraction can lead to machine imagination. I will present algorithms that enable association between different domains in an unsupervised manner. This ability to associate allows automatic creation of audio and visual content (images, videos, 4D space-time visualization of dynamic events) that is also user-controllable and interactive. I will show diverse user applications in audio-visual data retargeting, reconstruction, synthesis, and manipulation. These applications are the first steps towards building machines with a powerful audio-visual simulator that will enable them to imagine complex hypothetical situations, and model the aspects of their surroundings that is not easily perceived. Bio - Aayush Bansal is a PhD candidate at the Robotics Institute of Carnegie Mellon University. He is a recipient of Uber Presidential Fellowship (2016-17), Qualcomm Fellowship (2017-18), and Snap Fellowship (2019-20). The production houses such as BBC Studios and PBS are using his research work to create documentaries and short movies. Various national and international media such as NBC, CBS, France TV, and The Journalist have extensively covered his work. More details are here: http://www.cs.cmu.edu/~aayushb]]></description>
      <author>bmaneesh</author>
    </item>
    <item>
      <title><![CDATA[From Big Data to Bedside (BD2B): Towards AI-Based Precision Oncology]]></title>
      <pubDate>Thursday, Sep 26 12:31 PM -0400</pubDate>
      <link>http://halley.exp.sis.pitt.edu/comet/presentColloquium.do?col_id=18064</link>
      <content:encoded><![CDATA[<b>Speaker:</b>&nbsp;Xinghua Lu MD, PhD<br/><b>Date:</b>&nbsp;Friday, Sep 27 on 11:00 AM - 12:00 PM<br/><b>Location:</b>&nbsp;407A/B BAUM, Offices at Baum, 5607 Baum Blvd. or weblink in description<br/><b>Detail:<b><p>Cancer is mainly caused by somatic genome alterations that perturb cellular signaling pathways, and it is anticipated that precisely targeting patient-specific genomic alterations of individual tumors (precision oncology) will lead to more effective therapies.&nbsp;&nbsp; However, while genome-scale data from an individual patient can be readily obtained to guide contemporary molecularly targeted therapies,&nbsp;only a small fraction (&lt; 10%) of all patients benefit from current precision oncology approach, and majority of patient are treated by standard of cares following current guide lines, which are not personalized.&nbsp; In this presentation, I will discuss different artificial intelligence technologies that can advance precision oncology, including causal inference methods for revealing the disease mechanisms of each individual tumor, causal network methods for discovering cancer pathways, and deep learning methods to infer the state of signaling machinery of tumor cells.&nbsp;&nbsp; I will further discuss how information derived from such analyses can be used to guide personalized application of anti-cancer drugs and present our results from pre-clinical studies.&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <table align="center" border="1" cellpadding="0" style="width:600px"> <tbody> <tr> <td> <table border="0" cellpadding="0" cellspacing="0"> <tbody> <tr> <td style="height:30px; width:200px"> <p><a href="https://nam05.safelinks.protection.outlook.com/?url=https%3A%2F%2Fbluejeans.com%2F148972068%3Fsrc%3DhtmlEmail&amp;data=02%7C01%7Ctls18%40pitt.edu%7C375e6058ba2344fc848408d736ddd0a2%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C1%7C637038198627666381&amp;sdata=G3hjjP2ZWL7tRWchoBMLyXr8xhI5uCA5bRSv0FkYn7M%3D&amp;reserved=0" target="_blank"><strong>Join Meeting </strong></a></p> </td> <td style="height:30px">&nbsp;</td> </tr> <tr> <td style="width:201px"> <p>(Join from computer or phone)</p> </td> <td>&nbsp;</td> </tr> </tbody> </table> </td> </tr> </tbody> </table> <p>&nbsp;</p> <p><strong>Phone Dial-in</strong><br /> <a href="tel:+1.408.317.9253">+1.408.317.9253</a> (US (Primary, San Jose))<br /> (<a href="https://nam05.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.bluejeans.com%2Fpremium-numbers&amp;data=02%7C01%7Ctls18%40pitt.edu%7C375e6058ba2344fc848408d736ddd0a2%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C1%7C637038198627666381&amp;sdata=397%2BsYtMDEzQ3jMTgya1rab%2FeS6HrHNBMZtpa0PGpHw%3D&amp;reserved=0">Global Numbers</a>)<br /> <br /> <strong>Meeting ID: 148 972 068</strong></p> <p>&nbsp;</p> <p>&nbsp;</p>]]></content:encoded>
      <description><![CDATA[Speaker:�Xinghua Lu MD, PhDDate:�Friday, Sep 27 on 11:00 AM - 12:00 PMLocation:�407A/B BAUM, Offices at Baum, 5607 Baum Blvd. or weblink in descriptionDetail:Cancer is mainly caused by somatic genome alterations that perturb cellular signaling pathways, and it is anticipated that precisely targeting patient-specific genomic alterations of individual tumors (precision oncology) will lead to more effective therapies.�� However, while genome-scale data from an individual patient can be readily obtained to guide contemporary molecularly targeted therapies,�only a small fraction (< 10%) of all patients benefit from current precision oncology approach, and majority of patient are treated by standard of cares following current guide lines, which are not personalized.� In this presentation, I will discuss different artificial intelligence technologies that can advance precision oncology, including causal inference methods for revealing the disease mechanisms of each individual tumor, causal network methods for discovering cancer pathways, and deep learning methods to infer the state of signaling machinery of tumor cells.�� I will further discuss how information derived from such analyses can be used to guide personalized application of anti-cancer drugs and present our results from pre-clinical studies.� � � � Join Meeting � (Join from computer or phone) � � Phone Dial-in +1.408.317.9253 (US (Primary, San Jose)) (Global Numbers) Meeting ID: 148 972 068 � �]]></description>
      <author>tls18</author>
    </item>
    <item>
      <title><![CDATA[Expanding the reach of AIED systems: Adapting to social learning processes]]></title>
      <pubDate>Wednesday, Sep 25 8:25 AM -0400</pubDate>
      <link>http://halley.exp.sis.pitt.edu/comet/presentColloquium.do?col_id=18072</link>
      <content:encoded><![CDATA[<b>Speaker:</b>&nbsp;Erin Walker<br/><b>Date:</b>&nbsp;Friday, Sep 27 on 12:30 PM - 1:00 PM<br/><b>Location:</b>&nbsp;5317 Sennott Square<br/><b>Detail:<b><p>Artificial intelligence in education (AIED) systems are technologies that use artificial intelligence and machine learning to understand how students learn, and adapt the learning experience to each individual&#39;s cognitive, metacognitive, and motivational needs. In some cases, these systems have been demonstrated to be nearly as effective as human tutors and better than traditional forms of classroom instruction.&nbsp;However, they have primarily been used to improve individual problem-solving in well-defined domains such as math and science, through the explicit modeling and support of domain-related procedures and knowledge. My research explores ways to expand these systems to a broader set of learning processes and outcomes. In this talk, I present examples of novel applications of AIED systems, including EMBRACE, an intelligent tutoring system drawing from an embodied cognition theory of reading comprehension, and Nico, a teachable robot for mathematics learning. I discuss the implications of this work for the development of AIED systems that can better adapt to a student&#39;s social learning context.</p>]]></content:encoded>
      <description><![CDATA[Speaker:�Erin WalkerDate:�Friday, Sep 27 on 12:30 PM - 1:00 PMLocation:�5317 Sennott SquareDetail:Artificial intelligence in education (AIED) systems are technologies that use artificial intelligence and machine learning to understand how students learn, and adapt the learning experience to each individual's cognitive, metacognitive, and motivational needs. In some cases, these systems have been demonstrated to be nearly as effective as human tutors and better than traditional forms of classroom instruction.�However, they have primarily been used to improve individual problem-solving in well-defined domains such as math and science, through the explicit modeling and support of domain-related procedures and knowledge. My research explores ways to expand these systems to a broader set of learning processes and outcomes. In this talk, I present examples of novel applications of AIED systems, including EMBRACE, an intelligent tutoring system drawing from an embodied cognition theory of reading comprehension, and Nico, a teachable robot for mathematics learning. I discuss the implications of this work for the development of AIED systems that can better adapt to a student's social learning context.]]></description>
      <author>ISP Admin</author>
    </item>
    <item>
      <title><![CDATA[Deep Learning for Robotics]]></title>
      <pubDate>Friday, Sep 27 10:34 AM -0400</pubDate>
      <link>http://halley.exp.sis.pitt.edu/comet/presentColloquium.do?col_id=18086</link>
      <content:encoded><![CDATA[<b>Speaker:</b>&nbsp;PIETER ABBEEL<br/><b>Date:</b>&nbsp;Friday, Sep 27 on 3:00 PM - 4:00 PM<br/><b>Location:</b>&nbsp;Gates Hillman Centers, ASA Conference Room 6115<br/><b>Detail:<b><p>Programming robots remains notoriously difficult.� Equipping robots with the ability to learn would by-pass the need for what otherwise often ends up being time-consuming task specific programming.� This talk will describe recent progress in deep reinforcement learning (robots learning through their own trial and error), in apprenticeship learning (robots learning from observing people), and in meta-learning for action (robots learning to learn). � This work has led to new robotic capabilities in manipulation, locomotion, and flight, with the same approach underlying advances in each of these domains.</p> <br/> <br/> <p>Professor Pieter Abbeel is Director of the Berkeley Robot Learning Lab and Co-Director of the Berkeley Artificial Intelligence (BAIR) Lab. Abbeel?s research strives to build ever more intelligent systems, which has his lab push the frontiers of deep reinforcement learning, deep imitation learning, deep unsupervised learning, transfer learning, meta-learning, and learning to learn, as well as study the influence of AI on society.� His lab also investigates how AI could advance other science and engineering disciplines.� Abbeel?s Intro to AI class has been taken by over 100K students through edX, and his Deep RL and Deep Unsupervised Learning materials are standard references for AI researchers.� Abbeel has founded three companies: Gradescope (AI to help teachers with grading homework and exams), Covariant (AI for robotic automation of warehouses and factories), and Berkeley Open Arms (low-cost, highly capable 7-dof robot arms), advises many AI and robotics start-ups, and is a frequently sought after speaker worldwide for C-suite sessions on AI future and strategy.� Abbeel has received many awards and honors, including the PECASE, NSF-CAREER, ONR-YIP, Darpa-YFA, TR35.� His work is frequently featured in the press, including the New York Times, Wall Street Journal, BBC, Rolling Stone, Wired, and Tech Review.</p> <br/> <p><strong>Faculty Host</strong>: David Held</p>]]></content:encoded>
      <description><![CDATA[Speaker:�PIETER ABBEELDate:�Friday, Sep 27 on 3:00 PM - 4:00 PMLocation:�Gates Hillman Centers, ASA Conference Room 6115Detail:Programming robots remains notoriously difficult.� Equipping robots with the ability to learn would by-pass the need for what otherwise often ends up being time-consuming task specific programming.� This talk will describe recent progress in deep reinforcement learning (robots learning through their own trial and error), in apprenticeship learning (robots learning from observing people), and in meta-learning for action (robots learning to learn). � This work has led to new robotic capabilities in manipulation, locomotion, and flight, with the same approach underlying advances in each of these domains. Professor Pieter Abbeel is Director of the Berkeley Robot Learning Lab and Co-Director of the Berkeley Artificial Intelligence (BAIR) Lab. Abbeel?s research strives to build ever more intelligent systems, which has his lab push the frontiers of deep reinforcement learning, deep imitation learning, deep unsupervised learning, transfer learning, meta-learning, and learning to learn, as well as study the influence of AI on society.� His lab also investigates how AI could advance other science and engineering disciplines.� Abbeel?s Intro to AI class has been taken by over 100K students through edX, and his Deep RL and Deep Unsupervised Learning materials are standard references for AI researchers.� Abbeel has founded three companies: Gradescope (AI to help teachers with grading homework and exams), Covariant (AI for robotic automation of warehouses and factories), and Berkeley Open Arms (low-cost, highly capable 7-dof robot arms), advises many AI and robotics start-ups, and is a frequently sought after speaker worldwide for C-suite sessions on AI future and strategy.� Abbeel has received many awards and honors, including the PECASE, NSF-CAREER, ONR-YIP, Darpa-YFA, TR35.� His work is frequently featured in the press, including the New York Times, Wall Street Journal, BBC, Rolling Stone, Wired, and Tech Review. Faculty Host: David Held]]></description>
      <author>comet.paws</author>
    </item>
    <item>
      <title><![CDATA[Artificial Intelligence Healthcare - From Prevention & Diagnostics to Treatments (AI-PDT)]]></title>
      <pubDate>Monday, Sep 30 12:19 PM -0400</pubDate>
      <link>http://halley.exp.sis.pitt.edu/comet/presentColloquium.do?col_id=17958</link>
      <content:encoded><![CDATA[<b>Speaker:</b>&nbsp;NIH Artificial Intelligence Interest Group (AIIG), Office of Intramural Research and NIH AI Working Group for Autonomous Therapeutics (AIAT)<br/><b>Date:</b>&nbsp;Tuesday, Oct 01 on 9:00 AM - 5:30 PM<br/><b>Location:</b>&nbsp;Online - Webinar<br/><b>Detail:<b><p>We will explore how we can cooperate to apply AI to enhance, and revolutionize, healthcare. Our goals will include, but not be limited to, the following:<br /> <br /> 1. To strengthen and expand NIH?s innovation ecosystem by engaging a broad range of researchers, practitioners, and other professionals interested in artificial intelligence healthcare across the nation?s academic and industrial bases.<br /> 2. To inform stakeholders of NIH?s vision and priorities for AI, in the context of smart healthcare, to better enable collaboration and transition of technologies.<br /> 3. To inspire the technical research community to seek research and development partnerships with the Agency by sharing NIH?s record of innovative achievements.<br /> 4. To establish an AI Healthcare Task Force with Trans-NIH and Trans-Agency collaboration.</p>]]></content:encoded>
      <description><![CDATA[Speaker:�NIH Artificial Intelligence Interest Group (AIIG), Office of Intramural Research and NIH AI Working Group for Autonomous Therapeutics (AIAT)Date:�Tuesday, Oct 01 on 9:00 AM - 5:30 PMLocation:�Online - WebinarDetail:We will explore how we can cooperate to apply AI to enhance, and revolutionize, healthcare. Our goals will include, but not be limited to, the following: 1. To strengthen and expand NIH?s innovation ecosystem by engaging a broad range of researchers, practitioners, and other professionals interested in artificial intelligence healthcare across the nation?s academic and industrial bases. 2. To inform stakeholders of NIH?s vision and priorities for AI, in the context of smart healthcare, to better enable collaboration and transition of technologies. 3. To inspire the technical research community to seek research and development partnerships with the Agency by sharing NIH?s record of innovative achievements. 4. To establish an AI Healthcare Task Force with Trans-NIH and Trans-Agency collaboration.]]></description>
      <author>comet.paws</author>
    </item>
    <item>
      <title><![CDATA[Overcoming Mode Collapse and the Curse of Dimensionality]]></title>
      <pubDate>Friday, Sep 27 10:29 AM -0400</pubDate>
      <link>http://halley.exp.sis.pitt.edu/comet/presentColloquium.do?col_id=18074</link>
      <content:encoded><![CDATA[<b>Speaker:</b>&nbsp;Ke Li<br/><b>Date:</b>&nbsp;Tuesday, Oct 01 on 10:30 AM - 11:30 AM<br/><b>Location:</b>&nbsp;3305 Newell-Simon Hall<br/><b>Detail:<b><p><strong>Abstract:</strong></p> <p>&nbsp;</p> <p>In this talk, I will present our work on overcoming two long-standing problems in machine learning and algorithms:</p> <p>&nbsp;</p> <p>1. Mode collapse in generative adversarial nets (GANs)</p> <p>&nbsp;</p> <p>Generative adversarial nets (GANs) are perhaps the most popular class of generative models in use today. Unfortunately, they suffer from the well-documented problem of mode collapse, which the many successive variants of GANs have failed to overcome. I will illustrate why mode collapse happens fundamentally and show a simple way to overcome it, which is the basis of a new method known as Implicit Maximum Likelihood Estimation (IMLE).</p> <p>2. Curse of dimensionality in exact nearest neighbour search</p> <p>&nbsp;</p> <p>Efficient algorithms for exact nearest neighbour search developed over the past 40 years do not work in high (intrinsic) dimensions, due to the curse of dimensionality. It turns out that this problem is not insurmountable - I will explain how the curse of dimensionality arises and show a simple way to overcome it, which is the gives rise to a new family of algorithms known as Dynamic Continuous Indexing (DCI).</p> <p>&nbsp;</p> <p><strong>Bio:</strong></p> <p>&nbsp;</p> <p>Ke Li is a recent Ph.D. graduate from UC Berkeley, where he was advised by Prof. Jitendra Malik, and will join Google as a Research Scientist and the Institute for Advanced Study (IAS) as a Member hosted by Prof. Sanjeev Arora. He is interested in a broad range of topics in machine learning, computer vision, NLP and algorithms and has worked on generative modelling, nearest neighbour search and Learning to Optimize. He is particularly passionate about tackling long-standing fundamental problems that cannot be tackled with a straightforward application of conventional techniques. He received his Hon. B.Sc. in Computer Science from the University of Toronto in 2014.</p>]]></content:encoded>
      <description><![CDATA[Speaker:�Ke LiDate:�Tuesday, Oct 01 on 10:30 AM - 11:30 AMLocation:�3305 Newell-Simon HallDetail:Abstract: � In this talk, I will present our work on overcoming two long-standing problems in machine learning and algorithms: � 1. Mode collapse in generative adversarial nets (GANs) � Generative adversarial nets (GANs) are perhaps the most popular class of generative models in use today. Unfortunately, they suffer from the well-documented problem of mode collapse, which the many successive variants of GANs have failed to overcome. I will illustrate why mode collapse happens fundamentally and show a simple way to overcome it, which is the basis of a new method known as Implicit Maximum Likelihood Estimation (IMLE). 2. Curse of dimensionality in exact nearest neighbour search � Efficient algorithms for exact nearest neighbour search developed over the past 40 years do not work in high (intrinsic) dimensions, due to the curse of dimensionality. It turns out that this problem is not insurmountable - I will explain how the curse of dimensionality arises and show a simple way to overcome it, which is the gives rise to a new family of algorithms known as Dynamic Continuous Indexing (DCI). � Bio: � Ke Li is a recent Ph.D. graduate from UC Berkeley, where he was advised by Prof. Jitendra Malik, and will join Google as a Research Scientist and the Institute for Advanced Study (IAS) as a Member hosted by Prof. Sanjeev Arora. He is interested in a broad range of topics in machine learning, computer vision, NLP and algorithms and has worked on generative modelling, nearest neighbour search and Learning to Optimize. He is particularly passionate about tackling long-standing fundamental problems that cannot be tackled with a straightforward application of conventional techniques. He received his Hon. B.Sc. in Computer Science from the University of Toronto in 2014.]]></description>
      <author>comet.paws</author>
    </item>
  </channel>
</rss>